FLIP ROBO

DEEP LEARNING — WORKSHEET 3 

Q1 to Q8 are MCQs with only one correct answer. Choose the correct option.

1.Which of the following is true about model capacity (where model capacity means the ability of neural
network to approximate complex functions)?

A) As dropout ratio increases, model capacity increases

B) As number of hidden layers increase, model capacity increases

C) As learning rate increases, model capacity increases

D) None of the above

Ans: C

2. Batch Normalization is helpful because?

A) It is a very efficient backpropagation technique

B) It returns back the normalized mean and standard deviation of weights

C) It normalizes (changes) all the input before sending it to the next layer

D) None of the above

Ans: C

3. What if we use a learning rate that’s too large?

A) Network will not converge B) Network will converge
C) either A or B D) None of the above

4. What are the factors to select the depth of neural network?

i) Type of neural network (e.g. MLP, CNN etc.)

ii) Input data

iii) Computation power, i.e. Hardware capabilities and software capabilities

iv) Learning Rate

v) The output function to map

A) 1, 2,4,5 B) 253, 4,5

C)1,3,4,5 D) All of these

Ans: C

5. Suppose you have inputs as x, y, and z with values -2, 5, and -4 respectively. You have a neuron ‘q’ and
neuron ‘f? with functions:

qqxt+y
f=q*z
Graphical representation of the functions is as follows:

X -2

What is the gradient of F with respect to x, y, and z? (use chain rule of derivatives to find the solution)

A) (G3, -4, 4) B) (-3, 4, 4)

C) (-4, -4, 3) D) (4, 4, 3)

Ans:A

6. Which of the following statement is the best description of early stopping?

A) Train the network until a local minimum in the error function is reached

B) Simulate the network on a test dataset after every epoch of training. Stop training when the generalization
error starts to increase

C) Add a momentum term to the weight update in the Generalized Delta Rule, so that training converges more
quickly

D) None of the above

Ans: B

7. Which gradient descent technique is more advantageous when the data is too big to handle in RAM simultaneously?
A) Mini Batch Gradient Descent B) Stochastic Gradient Descent
C) Full Batch Gradient Descent D) either A or B

Ans:C

8. Consider the scenario. The problem you are trying to solve has a small amount of data. Fortunately, you have a
pre-trained neural network that was trained on a similar problem. Which of the following methodologies would
you choose to make use of this pre-trained network?

A) Freeze all the layers except the last, re-train the last layer

B) Assess on every layer how the model performs and only select a few of them
C) Fine tune the last couple of layers only

D) Re-train the model for the new dataset

Ans: B

Q9 and Q10 are MCQs with one or more correct answers. Choose all the correct options.

9. Which of the following neural network training challenge can be solved using batch normalization?

A) Overfitting B) Training is too slow
C) Restrict activations to become too high or low
D) None of these

Ans: A and B

10. For a binary classification problem, which of the following activations may be used in output layer?
A) ReLU B) sigmoid
C) softmax D) Leaky ReLU

Ans: A and C

Q11 to Q15 are subjective answer type question. Answer them briefly.

11. What will happen if we do not use activation function in artificial neural networks?

Ans: If we are not going to use activation function in artificial neural network, then  network would be less powerful and will not be able to learn
the complex patterns from the data. A neural network without an activation function is essentially just a linear regression model.

12. How does forward propagation and backpropagation work in deep learning?

Ans: In forward propagation the input data is fed in the forward direction through the network and each hidden layer accepts the input data, 
processes it as per the activation function and passes to the successive layer. Backpropagation is about understanding how changing the weights and 
biases in a network changes the cost function. Ultimately, this means computing the partial derivatives. Backpropagation will give us a procedure 
to compute the error.

13. Explain briefly the following variant of Gradient Descent: Stochastic, Batch, and. Mini-batch?

Ans: Gradient descent is the backbone of neural networks training and the entire field of deep learning. This method enables us to teach neural networks
to perform arbitrary tasks without explicitly program them for it. Stochastic gradient descent is an iterative method for optimizing an objective function
with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual 
gradient by an estimate.
Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates 
the model after all training examples have been evaluated. One cycle through the entire training dataset is called a training epoch.
Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate 
the model error and update model coefficients. Implementations may choose to sum the gradient over the mini-batch which further reduces the variance of the gradient.

14. What are the main benefits of Mini-batch Gradient Descent?

Ans: Benefits of Mini-batch Gradient Descent are,
1) It will easily fits in the memory.
2) It is computationally efficient.
Benefit from vectorization.
3) If stuck in local minimums, some noisy steps can lead the way out of them.
4) Average of the training samples produces stable error gradients and convergence.

15. What is transfer learning?

Ans:Transfer of learning means the use of previously acquired knowledge and skills in new learning or problem-solving situations. 
Thereby similarities and analogies between previous and actual learning content and processes may play a crucial role. Also transfer learning 
is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with 
comparatively little data.
